{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.load import dumps, loads\n",
    "from typing import List\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
    "os.environ['DeepSeek_API_KEY'] = 'sk-2b3073febb6b486e94e62f9e9704f759'\n",
    "os.environ['EMBEDDING_API_KEY'] = 'sk-zk23347d533c0c49e3d781430f5bb140c0a67d78ee90597e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "    parse_only=bs4.SoupStrainer(\n",
    "        class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "    )\n",
    "    ),\n",
    ")\n",
    "\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(splits, OpenAIEmbeddings(\n",
    "                                                            model=\"text-embedding-ada-002\",\n",
    "                                                            api_key=os.environ.get('EMBEDDING_API_KEY'),\n",
    "                                                            base_url=\"https://api.zhizengzeng.com/v1/\",\n",
    "                                                            encoding_format=\"float\"\n",
    "))\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model='deepseek-chat',\n",
    "    openai_api_key=os.environ.get('DeepSeek_API_KEY'),\n",
    "    openai_api_base='https://api.deepseek.com',\n",
    "    max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_union(documents: list[List]):\n",
    "    \"\"\"\n",
    "    Unique union of retrieved docs\n",
    "    \"\"\"\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EDY\\AppData\\Local\\Temp\\ipykernel_12544\\523189616.py:7: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is task decomposition for LLM agents\"\n",
    "retrievla_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrievla_chain.invoke({\"question\": question})\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals or steps. This is a crucial component of planning in LLM-powered autonomous agent systems, as it enables the agent to handle intricate tasks more efficiently.\\n\\nIn the context of LLM agents, task decomposition can be achieved through various methods:\\n\\n1. **Chain of Thought (CoT)**: This is a standard prompting technique where the model is instructed to \"think step by step.\" By decomposing a complex task into smaller, simpler steps, the model can utilize more test-time computation to tackle the problem effectively. This approach also provides insight into the model\\'s reasoning process.\\n\\n2. **Tree of Thoughts (ToT)**: This method extends CoT by exploring multiple reasoning possibilities at each step. It decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be either breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or majority vote.\\n\\n3. **Prompting Techniques**: Task decomposition can also be done using simple prompts like \"Steps for XYZ.\\\\\\\\n1.\" or \"What are the subgoals for achieving XYZ?\" These prompts guide the LLM to break down the task into smaller components.\\n\\n4. **Task-Specific Instructions**: For specific tasks, such as writing a novel, the agent can be given instructions like \"Write a story outline,\" which inherently involves breaking down the task into smaller parts.\\n\\n5. **Human Inputs**: In some cases, human intervention may be required to help the agent decompose a task effectively.\\n\\nOverall, task decomposition allows LLM agents to plan and execute complex tasks by dividing them into more manageable subgoals, thereby improving the efficiency and effectiveness of the agent\\'s problem-solving capabilities.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='deepseek-chat',\n",
    "    openai_api_key=os.environ.get('DeepSeek_API_KEY'),\n",
    "    openai_api_base='https://api.deepseek.com',\n",
    "    max_tokens=1024,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrievla_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reciprocal_rank_fusion(results: list[List], k=60):\n",
    "    \"\"\"\n",
    "    Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "    and an optional parameter k used in the RRF formula\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            \n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    \n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals or steps. This is a crucial component of planning in an LLM-powered autonomous agent system, as it enables the agent to handle intricate tasks efficiently.\\n\\nThere are several techniques and approaches to task decomposition:\\n\\n1. **Chain of Thought (CoT)**: This is a standard prompting technique where the model is instructed to \"think step by step.\" By decomposing a hard task into smaller, simpler steps, the model can utilize more test-time computation and provide insights into its reasoning process.\\n\\n2. **Tree of Thoughts (ToT)**: This approach extends CoT by exploring multiple reasoning possibilities at each step. It decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be either breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or majority vote.\\n\\n3. **Prompting Techniques**: Task decomposition can also be achieved through simple prompting, such as asking the model to list steps for a task (e.g., \"Steps for XYZ.\\\\n1.\") or by using task-specific instructions (e.g., \"Write a story outline.\").\\n\\n4. **Human Inputs**: In some cases, task decomposition can involve human inputs to guide the agent in breaking down the task effectively.\\n\\nOverall, task decomposition allows LLM agents to tackle complex tasks by dividing them into smaller, more manageable components, thereby improving the agent\\'s ability to plan and execute tasks effectively.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "question = \"What is task decomposition for LLM agents\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is the definition of task decomposition in the context of LLM (Large Language Model) agents?  ',\n",
       " '2. How do LLM agents use task decomposition to solve complex problems?  ',\n",
       " '3. What are the common techniques or algorithms used for task decomposition in LLM agents?']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foramat_qa_pair(question, answer):\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "         \"question\": itemgetter(\"question\"),\n",
    "         \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pairs = foramat_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\" + q_a_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The common techniques or algorithms used for task decomposition in LLM agents include:\\n\\n1. **Chain of Thought (CoT)**: This technique encourages the LLM to \"think step by step,\" breaking down complex tasks into smaller, more manageable steps. By decomposing the task into smaller parts, the agent can systematically address each step, enhancing its ability to plan and execute the task effectively.\\n\\n2. **Tree of Thoughts (ToT)**: This method extends CoT by exploring multiple reasoning possibilities at each step. The problem is first decomposed into multiple thought steps, and then multiple thoughts are generated per step, creating a tree-like structure. The agent can then use search algorithms like BFS (breadth-first search) or DFS (depth-first search) to navigate through these possibilities, with each state evaluated by a classifier or majority vote.\\n\\n3. **Prompting and Instructions**: Task decomposition can be facilitated through simple prompting (e.g., \"Steps for XYZ.\\\\\\\\n1.\" or \"What are the subgoals for achieving XYZ?\") or task-specific instructions (e.g., \"Write a story outline.\" for writing a novel). These prompts guide the LLM to break down the task into smaller, actionable steps.\\n\\n4. **Human Inputs**: In some cases, human inputs can be used to assist in the decomposition process, especially when the task requires domain-specific knowledge or expertise that the LLM may lack.\\n\\nThese techniques allow LLM agents to plan, execute, and refine tasks more effectively, ultimately improving their performance on complex problems. This approach not only makes the task more manageable but also provides insights into the model’s reasoning process, allowing for better understanding and optimization of the agent’s performance.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EDY\\AppData\\Local\\Temp\\ipykernel_12544\\1477651150.py:10: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(sub_question)\n"
     ]
    }
   ],
   "source": [
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def retrieval_ang_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\": question})\n",
    "\n",
    "    rag_results = []\n",
    "\n",
    "    for sub_question in sub_questions:\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "\n",
    "        answer = (prompt_rag \n",
    "                  | llm \n",
    "                  | StrOutputParser()).invoke(\n",
    "                      {\"context\": retrieved_docs, \n",
    "                       \"question\": question}\n",
    "        )\n",
    "\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results, sub_questions\n",
    "\n",
    "answer, question =  retrieval_ang_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
