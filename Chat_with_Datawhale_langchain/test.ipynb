{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import requests\n",
    "import _thread as thread\n",
    "import base64\n",
    "import datetime\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import hashlib\n",
    "import hmac\n",
    "import os\n",
    "import queue\n",
    "from urllib.parse import urlparse\n",
    "import ssl\n",
    "from datetime import datetime\n",
    "from time import mktime\n",
    "from urllib.parse import urlencode\n",
    "from wsgiref.handlers import format_date_time\n",
    "import zhipuai\n",
    "from langchain.utils import get_from_dict_or_env\n",
    "\n",
    "import websocket  # 使用websocket_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_key: sk-2b3073febb6b486e94e62f9e9704f759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好！很高兴见到你。有什么我可以帮忙的吗？'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_llm_api_key(model:str, env_file:dict()=None):\n",
    "    \"\"\"\n",
    "    通过 model 和 env_file 的来解析平台参数\n",
    "    \"\"\"   \n",
    "    if env_file == None:\n",
    "        _ = load_dotenv(find_dotenv())\n",
    "        env_file = os.environ\n",
    "    if model == \"openai\":\n",
    "        return env_file[\"OPENAI_API_KEY\"]\n",
    "    elif model == \"wenxin\":\n",
    "        return env_file[\"wenxin_api_key\"], env_file[\"wenxin_secret_key\"]\n",
    "    elif model == \"spark\":\n",
    "        return env_file[\"spark_api_key\"], env_file[\"spark_appid\"], env_file[\"spark_api_secret\"]\n",
    "    elif model == \"zhipuai\":\n",
    "        return get_from_dict_or_env(env_file, \"zhipuai_api_key\", \"ZHIPUAI_API_KEY\")\n",
    "        # return env_file[\"ZHIPUAI_API_KEY\"]\n",
    "    elif model == \"deepseek\":\n",
    "        return env_file[\"DeepSeek_API_for_RAG\"]\n",
    "    else:\n",
    "        raise ValueError(f\"model{model} not support!!!\")\n",
    "\n",
    "\n",
    "def get_completion_deepseek(prompt : str, model : str, temperature : float, api_key:str, max_tokens:int, base_url=\"https://api.deepseek.com\"):\n",
    "    # 封装 OpenAI 原生接口\n",
    "    if api_key == None:\n",
    "        api_key = parse_llm_api_key(\"deepseek\")\n",
    "    print(\"api_key:\",api_key)\n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    # 具体调用\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # 模型输出的温度系数，控制输出的随机程度\n",
    "        max_tokens = max_tokens, # 回复最大长度\n",
    "    )\n",
    "    # 调用 OpenAI 的 ChatCompletion 接口\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "llm = get_completion_deepseek(\"你好\", \"deepseek-chat\", 0.1, None, 2048)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
